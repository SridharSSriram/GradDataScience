---
title: "Sridhar_Sriram_HW10"
output:
  rmarkdown::github_document
---

## Lab 10.4, Lab 1 (PCA)

### <span style = "color: blue">Exploring the `USArrests Dataset`</span>
```{r}
states = row.names(USArrests)

names(USArrests)
```

#### <span style = "color:green">Using the `apply` function:</span>

* Allows us to apply any given function onto a specified dataset

* Syntax is `apply(<dataset>, <margin:1,2>,<function>)`

  * **margin**:
  
    * **1**:  applying to rows
    
    * **2**: applying to columns
    
```{r}
apply(USArrests,2,mean)

apply(USArrests,2,var)
```

#### <span style = "color:green">Standardizing dataset to avoid overwhelming influence by the Assault data points (due to very high variance and very high mean)</span>

`prcomp` --> PCA function in R

* Centers the variables to have a mean of 0


```{r}
pr.out = prcomp(USArrests, scale = TRUE)

names(pr.out)

pr.out$center
pr.out$sdev
pr.out$scale

```

The `rotation` matrix provides PCA loadings; each column has the respective PC loading vector

```{r}
pr.out$rotation
```

We expect to see 4 Principal components because the rule of thumb is 

* number of principal components = min(observations -1, variables)

```{r}
dim(pr.out$x)
```

** Plotting the first two principal components** :

```{r}
biplot(pr.out,scale = 0)
```

Fixing the problem of principal components being unique up to a sign change

```{r}
pr.out$rotation = -pr.out$rotation
pr.out$rotation

pr.out$x = -pr.out$x

biplot(pr.out,scale = 0)
```

** Obtaining the variance related to each principal component**

```{r}
pr.var = pr.out$sdev^2

pr.var
```

#### <span style = "color:green">Variance explained by each principal component </span>

```{r}
variance.by.component = pr.var/sum(pr.var)
variance.by.component
```

#### <span style = "color:green">Visualizing these effects</span>

```{r}
plot(variance.by.component,
     xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained",
     ylim = c(0,1),
     type = 'b')

plot(cumsum(variance.by.component), 
     xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained",
     ylim = c(0,1),
     type = 'b')

```

`cumsum()` calculates cumulative sums of elements in a numeric vector

```{r}
a = c(1,2,8,-3)

cumsum(a)
```


## Lab 10.5.1, Lab 2 (K-means Clustering)


### <span style = "color:blue">Simulated 2-Cluster K-Means</span>

```{r}
set.seed(2)
x = matrix(rnorm(50*2), ncol = 2)
x[1:25,1] = x[1:25,1] + 3

x[1:25,2] = x[1:25,2] - 4
```

#### <span style = "color:purple">Performing the clustering with K = 2</span>

```{r}
km.out = kmeans(x,2,nstart = 20)
km.out
```

#### <span style = "color:purple">Plotting the kmeans clustering with colored distinction</span>

```{r}
plot(x,
     col = (km.out$cluster+1),
     main= "K-Means Clustering Results with K  = 2",
     ylab = "",
     pch = 20,
     cex = 2)
```


<span style = "color:red"> **FOR FUTURE APPLICATIONS:** </span>

If we have clusters/components that are more than just 2 in number, then we can perform PCA and plot/visualize clusters relating to the first two most significant cluster


#### <span style = "color:purple">Same data, now with 3 clusters</span>

```{r}
set.seed(1234)

km.out.3 = kmeans(x,3,nstart = 20)
km.out.3

plot(x,
     col = (km.out.3$cluster+1),
     main= "K-Means Clustering Results with K  = 3",
     ylab = "",
     pch = 20,
     cex = 3)
```

**Within kmeans**:
`nstart` :

* parameter for multiple initial cluster assignments

** kmeans components: **

`tot.withinss` :

* Total within-cluster sum of squares --> minimize this!!!

`withinss`:

* individual within-cluster sum of squares

## Lab 10.5.2, Lab (hierarchial clustering)

### <span style = "color:blue">Using same data from previous lab (x)</span>

#### <span style = "color: red ">Complete, average, single linkage</span>

```{r}
hc.complete = hclust(dist(x),method = "complete")

hc.single = hclust(dist(x),method = "single")

hc.average = hclust(dist(x),method = "average")
```

#### <span style = "color: red ">Plotting the associated dendrograms</span>

```{r}
par(mfrow = c(1,3))

plot(hc.complete,
     main = "Complete Linkage",
     sub = "",
     xlab = "",
     cex = 0.9)

plot(hc.average,
     main = "Average Linkage",
     sub = "",
     xlab = "",
     cex = 0.9)

plot(hc.single,
     main = "Single Linkage",
     sub = "",
     xlab = "",
     cex = 0.9)
```

#### <span style = "color: red ">Determining cluster labels for each observation</span>

```{r}
cutree(hc.complete,2)

cutree(hc.average,2)

cutree(hc.single,2)
```


The `cutree` using `hc.single` returns a list of `1`s because single linkage identifies one point as belonging to its own cluster

** Fixing the above (using 4 clusters) **

```{r}
cutree(hc.single, 4)
```


#### <span style = "color: red ">Scaling before performing hierarchial clustering</span>

```{r}
scaled = scale(x)

plot(hclust(dist(scaled), method = "complete"),
     main = "Hierarchial Clustering with Scaled Features")
```


#### <span style = "color: red ">Correlation-based distance</span>

```{r}
x = matrix(rnorm(30*3),ncol = 3)

dd = as.dist(1-cor(t(x)))

plot(hclust(dd,method = "complete"),
     main = "Complete Linkage w/ Correlation-Based Distance",
     xlab = "",
     sub = "")
```


