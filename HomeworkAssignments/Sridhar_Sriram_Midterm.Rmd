---
title: "Midterm, Sridhar Sriram"
output:
  rmarkdown::github_document
---

## Question 1:

The code sample does the following:

- assigns the length of vector `myvector` to the variable `n`
- assigns the values from 1 to `n` into the vector `count`
- and then checks to see if the values subsetted in pages in `myvector` are equivalent to `value` 

## Question 2: 
Below is the code to import the dataset `Midterm_Files_2.csv`

```{r}
library(readr)
Midterm_File_2 <- read_csv("~/Downloads/Midterm File 2.csv")
View(Midterm_File_2)
```

```{r}
#finding and storing the max value in the Midterm_File_2.csv file
max_midterm_file_value = max(Midterm_File_2[,1])
max_midterm_file_value

#finding and storing the indices where the max value appears
indices = which(Midterm_File_2[,1] == max_midterm_file_value)
indices

```
As indicated by the above code:

* The maximum value of `Midter_File_2.csv` is 1
* This value is found at indices `12, 130, 261, 442, 605, 771, 999, 1232`

## Question 3: 

using the `scan` function with the `text` parameter set to the input
```{r}
beverages <- scan(text = "3 4 1 1 3 4 3 3 1 3 2 1 2 1 2 3 2 3 1 1 1 1 4 3 1")
beverages
```


## Question 4:

```{r}
library(ggplot2)

bev.labeled.fact <- factor(beverages, labels = c("soda","diet soda", "ice tea", "water"))
bev.labeled.fact

ggplot(data = as.data.frame(bev.labeled.fact),
       aes(bev.labeled.fact)) + 
  geom_bar(color=c("maroon","navy","darkgreen","gold"), 
           fill =c("maroon","navy","darkgreen","gold") ) +
  ggtitle("Bar of Frequencies of Beverage Preferences") +
  xlab("beverages")+
  ylab("counts")
```


## Question 5: 
```{r}
stemmed.data <-c(2,3,16,23,14,12,4,13,2,0,0,6,28,31,14,4,8,2,5)

stem(stemmed.data)
```



## Question 6: 

When called, the `apropos()` returns a character vector containing the elements in the searched data that matches what is specified in the `what` parameter of the function (given in a regular expression format).


## Question 7: 

```{r}
sals <- c(12,.4,5,2,50,8,3,1,4,.25)
sals.cut <- cut(sals, 
                breaks = c(0,1,5,50),
                labels =c("poor","rich","very rich"))
sals.cut
```

```{r}

sals.table <- table(sals.cut)
sals.table
```


## Question 8: 

```{r}
numbers<- rnorm(100,mean = 5, sd = 3)
ggplot(data.frame(numbers), aes(numbers))+
  geom_histogram(binwidth = 2)
```

## Question 9: 

You can make R report *only* `x` number of decimals by using the `round` function with the following syntax:

* `round(<number>,<x = # decimal places>)`

for x= 3:

```{r}
round(1.453363536553, 3)
```

for x = 1:

```{r}
round(1.453363536553, 2)
```

for x = 6
```{r}
round(1.453363536553, 6)
```


## Question 10:

found that the `prop.table` function works perfectly for this exercise
```{r}
smokes <- (c("Y","N","N","Y","N","Y","Y","Y","N","Y"))
study <- c(1,2,2,3,3,1,2,1,3,2)

tmp.rows.addto1 <- table(x=smokes, 
                     y = study)

prop.table(tmp.rows.addto1,1)
```


## Question 11: 

```{r}
tmp.cols.addto1 <- table(x=smokes, 
                     y = study)
prop.table(tmp.cols.addto1,2)
```

## Question 12: 
```{r}
tmp.table.addto1 <- table(x=smokes, 
                     y = study)
prop.table(tmp.table.addto1)
```

## Question 13: 
Because there is no "Acceptance Rate" column, I created a vector that calculated all of the acceptance rates, and used this vector as the basis for my answer to this question. (The ouputted values are the answers to this questions, as more than one college has a 100% acceptance rate)

```{r}
library(readr)
College <- read_csv("~/Downloads/College.csv")
Collegecopy<- College
college.acceptancerate <- College$Accept / College$Apps

maximum.acceptancerates <- which(college.acceptancerate == max(college.acceptancerate))


Collegecopy[maximum.acceptancerates,1]

```


## Question 14: 

```{r}
Collegecopy$acceptancerates <- college.acceptancerate


College.privates <- Collegecopy[which(College$Private=="Yes"),]
College.publics <- Collegecopy[which(College$Private=="No"),]


t.test(College.privates$acceptancerates, College.publics$acceptancerates, conf.level = 0.95)
```

Because of a statistically significant p-value, at a confidence level of 95%, private schools *do not* accept at the the same rate as public schools.

## Question 15:

Surprisingly, the number of out of state students has the highest correlation with graduation rates with a 0.5713 correlation
```{r}
attach(Collegecopy)


#I type-casted the "Yes" values in the Private column to 1s and the "No" values to 0
things<-ifelse(Collegecopy[,2]=="Yes",1,0)

#altered the College dataset, adding in the typecasted Private column
Collegecopy2<-Collegecopy
Collegecopy2$Private <- things
Collegecopy2

##filtered out the column containing the college names because obviously this variable cannot be correlated to the graduation rate
cordata <- Collegecopy2[,2:length(Collegecopy)]

#temporary vector that holds the correlation matrix for the entire dataset
storage<-round(cor(cordata),4)
storage

#The correlation matrix pertaining to the Graduation Rate column
grad.rate.correlations<-data.frame(storage[-18,"Grad.Rate"])
grad.rate.correlations

grad.rate.correlations[which(grad.rate.correlations == max(abs(grad.rate.correlations))),]
```


## Question 16: 

Instead of creating my own data, I decided to randomly generate variables to demonstrate heteroscedasticity. The variability in variance in increases as values increase, and heteroscedasticity is further supported by the plotting of the `lm` function's support plots

```{r}
set.seed(123)
x <- 1:1000
y <- rnorm(n=1000, mean = x, sd=0.4*x)
plot(x,y)   
abline(0,1, col="red")

par(mfrow=c(2,2))
plot(lm(x~y))
```

## Question 17: 

```{r}
eight.values <- 1:8
choose <- function(n) {
  mean(sample(eight.values, size = n, replace = TRUE))
}

plot(sapply(1:1000, choose), type = "l", xlab = "# of dice", ylab = "average")
```

The law of large numbers dictates that after a large number of iterations/trials have been conducted, the results will eventually converge/even out to a theoretical point. This is shown by the high variance in the initial results being reduced over time.

## Question 18:
```{r}
set.seed(1648) # for reproducability
# create an empty vector for the means of exponential samples
clt <- NULL 
n <- 40 # sample size
lambda <- 0.2 # always use lambda = 0.2 for these sims

# take the mean of 40 samples of the exponential distribution. repeat 1,000 times
for (i in 1:1000) {
      clt <- c(clt, mean(rexp(n, lambda))) 
}
hist(clt, xlab='Sample Mean', main="Histogram of Sample Means from an Exponential Distribution (n=40)", col='beige')
sd(clt)
```

Because the CLT states that when independent random variables are added to a distribution, over time the distribution becomes normally disributed, the code above does the same. The standard deviation is rought 0.78, with normal distribution and variance
  
## Question 19: 

```{r}
site1 <- c(93,120,65,105,115,82,99,87,100,90,78,95,93,88,110)
site2<- c(85,45,80,28,75,70,65,55,50,40)
site3<- c(100, 75, 65,40,73,65,50,30,45,50)
site4<-c(96,58,95,90,65,80,85,95,82)

siteData <- data.frame(
       Y=c(site1, site2, site3, site4),
       Site =factor(rep(c("site1", "site2", "site3", "site4"), times=c(length(site1), length(site2), length(site3), length(site4))))
       )

test_set <- aov(Y~Site, data=siteData)
anova(test_set)
```

Due to a very low p-value, we can conclude that there is a difference between the means.

## Question 20:

```{r}
boxplot(site1,site2, site3, site4)
```


Yes, this plot supports my conclusion in 19 because there is definitely a difference between the means.
